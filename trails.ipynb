{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor(4.)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 5.])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector\n",
    "t2 =  torch.tensor([1.,2,3,5])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matix\n",
    "t3 = torch.tensor([[5.,6], [7,8],[9,10]])\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11, 13, 14],\n",
       "         [24, 66, 77]],\n",
       "\n",
       "        [[11, 13, 14],\n",
       "         [24, 66, 77]]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4 = torch.tensor([\n",
    "    [[11,13,14], [24,66,77]],\n",
    "    [[11,13,14], [24,66,77]]\n",
    "])\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([4])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "print(t3.shape)\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors operations and gradients\n",
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)\n",
    "x, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x*w+b\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derivatives\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: None\n",
      "dy/dw: tensor(3.)\n",
      "dy/db: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Display gradients\n",
    "print('dy/dx:', x.grad)\n",
    "print('dy/dw:', w.grad)\n",
    "print('dy/db:', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interoperability with Numpy\n",
    "\n",
    "import numpy as np\n",
    "x = np.array([[1,2], [3,4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.from_numpy(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('int32'), torch.int32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y.numpy()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67., 77., 85.],\n",
       "       [45., 66., 74.],\n",
       "       [36., 75., 86.],\n",
       "       [66., 46., 74.]], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array([[67,77,85],[45,66,74],[36,75,86],[66,46,74]], dtype='float32')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 56.,  70.],\n",
       "       [ 83., 101.],\n",
       "       [ 54.,  78.],\n",
       "       [ 56.,  75.]], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = np.array([[56, 70], [83, 101], [54,78], [56,75]], dtype='float32')\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  tensor([[67., 77., 85.],\n",
      "        [45., 66., 74.],\n",
      "        [36., 75., 86.],\n",
      "        [66., 46., 74.]])\n",
      "targets:  tensor([[ 56.,  70.],\n",
      "        [ 83., 101.],\n",
      "        [ 54.,  78.],\n",
      "        [ 56.,  75.]])\n"
     ]
    }
   ],
   "source": [
    "# convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(\"inputs: \",inputs)\n",
    "print(\"targets: \",targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9673,  0.1488,  0.4370],\n",
      "        [ 0.3897, -0.3826,  0.7551]], requires_grad=True)\n",
      "tensor([-0.7476,  0.6856], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(2,3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9673,  0.1488,  0.4370],\n",
       "        [ 0.3897, -0.3826,  0.7551]], requires_grad=True)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[67., 77., 85.],\n",
       "        [45., 66., 74.],\n",
       "        [36., 75., 86.],\n",
       "        [66., 46., 74.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9673,  0.3897],\n",
       "        [ 0.1488, -0.3826],\n",
       "        [ 0.4370,  0.7551]], grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-16.9535,  61.5194],\n",
       "        [ -2.1172,  48.8488],\n",
       "        [ 13.1712,  50.9598],\n",
       "        [-25.4050,  64.6837]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(inputs):\n",
    "    return inputs @ w.t()+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-16.9535,  61.5194],\n",
      "        [ -2.1172,  48.8488],\n",
      "        [ 13.1712,  50.9598],\n",
      "        [-25.4050,  64.6837]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 83., 101.],\n",
      "        [ 54.,  78.],\n",
      "        [ 56.,  75.]])\n"
     ]
    }
   ],
   "source": [
    "# compare to target\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5322.2095,   71.9213],\n",
       "        [7244.9380, 2719.7520],\n",
       "        [1666.9938,  731.1701],\n",
       "        [6626.7812,  106.4257]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difff = preds - targets\n",
    "muilti = difff * difff\n",
    "muilti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24490.1914, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(muilti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3061.2739, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(muilti) / difff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse loss\n",
    "def mse(t1,t2):\n",
    "    diff = t1-t2\n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3061.2739, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9673,  0.1488,  0.4370],\n",
      "        [ 0.3897, -0.3826,  0.7551]], requires_grad=True)\n",
      "tensor([[-3890.1821, -4510.4868, -5508.7427],\n",
      "        [-1142.3323, -1649.3879, -1917.2263]])\n"
     ]
    }
   ],
   "source": [
    "# gradients for weights\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gredient zero \n",
    "w.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    Adjust weights and biases using gradient descent\n",
    "\n",
    "1. Generate predictions\n",
    "2. Calculate loss\n",
    "3. Compute gradient w.r.t weights and biases\n",
    "4. Adjust the weights by subtractung a small quantity proportional to the gradient \n",
    "5. Reset the gradients to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(2,3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "def model(inputs):\n",
    "    return inputs @ w.t()+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[126.5822, -37.5669],\n",
      "        [108.2689, -25.6196],\n",
      "        [123.4699, -20.7884],\n",
      "        [122.4886, -37.8922]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse loss\n",
    "def mse(t1,t2):\n",
    "    diff = t1-t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8121.7627, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define loss\n",
    "\n",
    "loss = mse(preds, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3188.8169,  3842.8223,  4690.9868],\n",
      "        [-5978.0322, -7310.4282, -8840.7148]])\n",
      "tensor([  57.9524, -111.4668])\n"
     ]
    }
   ],
   "source": [
    "# compute gradients\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1912, -0.5103,  1.7928],\n",
      "        [-0.5451,  0.0413, -0.0372]], requires_grad=True)\n",
      "tensor([ 0.6765, -1.0621], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the weights and reset gradients\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1593, -0.5487,  1.7459],\n",
      "        [-0.4853,  0.1144,  0.0512]], requires_grad=True)\n",
      "tensor([ 0.6760, -1.0610], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6125.4102, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "\n",
    "loss = mse(preds, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(6125.4102, grad_fn=<DivBackward0>)\n",
      "loss: tensor(4637.9453, grad_fn=<DivBackward0>)\n",
      "loss: tensor(3529.6409, grad_fn=<DivBackward0>)\n",
      "loss: tensor(2703.8398, grad_fn=<DivBackward0>)\n",
      "loss: tensor(2088.5269, grad_fn=<DivBackward0>)\n",
      "loss: tensor(1630.0436, grad_fn=<DivBackward0>)\n",
      "loss: tensor(1288.4106, grad_fn=<DivBackward0>)\n",
      "loss: tensor(1033.8405, grad_fn=<DivBackward0>)\n",
      "loss: tensor(844.1392, grad_fn=<DivBackward0>)\n",
      "loss: tensor(702.7702, grad_fn=<DivBackward0>)\n",
      "loss: tensor(597.4128, grad_fn=<DivBackward0>)\n",
      "loss: tensor(518.8868, grad_fn=<DivBackward0>)\n",
      "loss: tensor(460.3524, grad_fn=<DivBackward0>)\n",
      "loss: tensor(416.7134, grad_fn=<DivBackward0>)\n",
      "loss: tensor(384.1729, grad_fn=<DivBackward0>)\n",
      "loss: tensor(359.9015, grad_fn=<DivBackward0>)\n",
      "loss: tensor(341.7915, grad_fn=<DivBackward0>)\n",
      "loss: tensor(328.2724, grad_fn=<DivBackward0>)\n",
      "loss: tensor(318.1738, grad_fn=<DivBackward0>)\n",
      "loss: tensor(310.6239, grad_fn=<DivBackward0>)\n",
      "loss: tensor(304.9731, grad_fn=<DivBackward0>)\n",
      "loss: tensor(300.7374, grad_fn=<DivBackward0>)\n",
      "loss: tensor(297.5562, grad_fn=<DivBackward0>)\n",
      "loss: tensor(295.1607, grad_fn=<DivBackward0>)\n",
      "loss: tensor(293.3508, grad_fn=<DivBackward0>)\n",
      "loss: tensor(291.9772, grad_fn=<DivBackward0>)\n",
      "loss: tensor(290.9288, grad_fn=<DivBackward0>)\n",
      "loss: tensor(290.1230, grad_fn=<DivBackward0>)\n",
      "loss: tensor(289.4979, grad_fn=<DivBackward0>)\n",
      "loss: tensor(289.0074, grad_fn=<DivBackward0>)\n",
      "loss: tensor(288.6176, grad_fn=<DivBackward0>)\n",
      "loss: tensor(288.3027, grad_fn=<DivBackward0>)\n",
      "loss: tensor(288.0438, grad_fn=<DivBackward0>)\n",
      "loss: tensor(287.8267, grad_fn=<DivBackward0>)\n",
      "loss: tensor(287.6408, grad_fn=<DivBackward0>)\n",
      "loss: tensor(287.4784, grad_fn=<DivBackward0>)\n",
      "loss: tensor(287.3334, grad_fn=<DivBackward0>)\n",
      "loss: tensor(287.2015, grad_fn=<DivBackward0>)\n",
      "loss: tensor(287.0796, grad_fn=<DivBackward0>)\n",
      "loss: tensor(286.9652, grad_fn=<DivBackward0>)\n",
      "loss: tensor(286.8564, grad_fn=<DivBackward0>)\n",
      "loss: tensor(286.7519, grad_fn=<DivBackward0>)\n",
      "loss: tensor(286.6507, grad_fn=<DivBackward0>)\n",
      "loss: tensor(286.5521, grad_fn=<DivBackward0>)\n",
      "loss: tensor(286.4555, grad_fn=<DivBackward0>)\n",
      "loss: tensor(286.3604, grad_fn=<DivBackward0>)\n",
      "loss: tensor(286.2668, grad_fn=<DivBackward0>)\n",
      "loss: tensor(286.1740, grad_fn=<DivBackward0>)\n",
      "loss: tensor(286.0821, grad_fn=<DivBackward0>)\n",
      "loss: tensor(285.9911, grad_fn=<DivBackward0>)\n",
      "loss: tensor(285.9006, grad_fn=<DivBackward0>)\n",
      "loss: tensor(285.8107, grad_fn=<DivBackward0>)\n",
      "loss: tensor(285.7213, grad_fn=<DivBackward0>)\n",
      "loss: tensor(285.6325, grad_fn=<DivBackward0>)\n",
      "loss: tensor(285.5440, grad_fn=<DivBackward0>)\n",
      "loss: tensor(285.4560, grad_fn=<DivBackward0>)\n",
      "loss: tensor(285.3684, grad_fn=<DivBackward0>)\n",
      "loss: tensor(285.2812, grad_fn=<DivBackward0>)\n",
      "loss: tensor(285.1942, grad_fn=<DivBackward0>)\n",
      "loss: tensor(285.1078, grad_fn=<DivBackward0>)\n",
      "loss: tensor(285.0216, grad_fn=<DivBackward0>)\n",
      "loss: tensor(284.9359, grad_fn=<DivBackward0>)\n",
      "loss: tensor(284.8505, grad_fn=<DivBackward0>)\n",
      "loss: tensor(284.7654, grad_fn=<DivBackward0>)\n",
      "loss: tensor(284.6807, grad_fn=<DivBackward0>)\n",
      "loss: tensor(284.5963, grad_fn=<DivBackward0>)\n",
      "loss: tensor(284.5123, grad_fn=<DivBackward0>)\n",
      "loss: tensor(284.4286, grad_fn=<DivBackward0>)\n",
      "loss: tensor(284.3452, grad_fn=<DivBackward0>)\n",
      "loss: tensor(284.2621, grad_fn=<DivBackward0>)\n",
      "loss: tensor(284.1795, grad_fn=<DivBackward0>)\n",
      "loss: tensor(284.0971, grad_fn=<DivBackward0>)\n",
      "loss: tensor(284.0151, grad_fn=<DivBackward0>)\n",
      "loss: tensor(283.9334, grad_fn=<DivBackward0>)\n",
      "loss: tensor(283.8520, grad_fn=<DivBackward0>)\n",
      "loss: tensor(283.7710, grad_fn=<DivBackward0>)\n",
      "loss: tensor(283.6903, grad_fn=<DivBackward0>)\n",
      "loss: tensor(283.6099, grad_fn=<DivBackward0>)\n",
      "loss: tensor(283.5298, grad_fn=<DivBackward0>)\n",
      "loss: tensor(283.4500, grad_fn=<DivBackward0>)\n",
      "loss: tensor(283.3706, grad_fn=<DivBackward0>)\n",
      "loss: tensor(283.2915, grad_fn=<DivBackward0>)\n",
      "loss: tensor(283.2125, grad_fn=<DivBackward0>)\n",
      "loss: tensor(283.1342, grad_fn=<DivBackward0>)\n",
      "loss: tensor(283.0558, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.9780, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.9005, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.8232, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.7462, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.6696, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.5932, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.5172, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.4414, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.3660, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.2908, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.2159, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.1415, grad_fn=<DivBackward0>)\n",
      "loss: tensor(282.0671, grad_fn=<DivBackward0>)\n",
      "loss: tensor(281.9932, grad_fn=<DivBackward0>)\n",
      "loss: tensor(281.9195, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# train for the 100 epochs\n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    print(\"loss:\",loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(281.8460, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 83., 101.],\n",
       "        [ 54.,  78.],\n",
       "        [ 56.,  75.]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[60.0901, 87.9207],\n",
       "        [54.2632, 76.7488],\n",
       "        [65.5553, 89.5514],\n",
       "        [67.4017, 64.7040]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input temp, rainfall, humidity\n",
    "inputs = np.array([[45,55,65], \n",
    "                   [23,23,23],\n",
    "                   [22,22,75],\n",
    "                   [75,45,23],\n",
    "                   [23,67,22],\n",
    "                   [22,75,45],\n",
    "                   [45,23,75],\n",
    "                   [67,22,23],\n",
    "                   [43,45,22],\n",
    "                   [75,67,75],\n",
    "                   [23,23,23],\n",
    "                   [22,22,22],\n",
    "                   [45,45,45],\n",
    "                   [67,67,67]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array([[75.,75], \n",
    "                   [23,23],\n",
    "                   [22,22],\n",
    "                   [75,45],\n",
    "                   [23,67],\n",
    "                   [22,75],\n",
    "                   [45,23],\n",
    "                   [67,22],\n",
    "                   [43,45],\n",
    "                   [75,67],\n",
    "                   [23,23],\n",
    "                   [22,22],\n",
    "                   [45,45],\n",
    "                   [67,67]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "inputs = torch.tensor(inputs)\n",
    "targets = torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[45., 55., 65.],\n",
       "         [23., 23., 23.],\n",
       "         [22., 22., 75.],\n",
       "         [75., 45., 23.]]),\n",
       " tensor([[75., 75.],\n",
       "         [23., 23.],\n",
       "         [22., 22.],\n",
       "         [75., 45.]]))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[75., 67., 75.],\n",
      "        [45., 55., 65.],\n",
      "        [43., 45., 22.],\n",
      "        [23., 23., 23.],\n",
      "        [45., 45., 45.]])\n",
      "tensor([[75., 67.],\n",
      "        [75., 75.],\n",
      "        [43., 45.],\n",
      "        [23., 23.],\n",
      "        [45., 45.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb  in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2661, -0.5118,  0.4204],\n",
      "        [-0.4302,  0.0139, -0.4823]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1617, 0.3038], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = nn.Linear(3,2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.2661, -0.5118,  0.4204],\n",
       "         [-0.4302,  0.0139, -0.4823]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1617, 0.3038], requires_grad=True)]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 11.3144, -49.6443],\n",
       "        [  4.1803, -20.3658],\n",
       "        [ 26.2869, -45.0299],\n",
       "        [  6.7582, -42.4325],\n",
       "        [-18.7585, -19.2733],\n",
       "        [-13.4496, -29.8254],\n",
       "        [ 31.8954, -54.9113],\n",
       "        [ 16.4004, -39.3096],\n",
       "        [ -2.1773, -28.1829],\n",
       "        [ 17.3600, -67.2079],\n",
       "        [  4.1803, -20.3658],\n",
       "        [  4.0056, -19.4671],\n",
       "        [  8.0241, -40.1367],\n",
       "        [ 11.8680, -59.9076]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[75., 75.],\n",
       "        [23., 23.],\n",
       "        [22., 22.],\n",
       "        [75., 45.],\n",
       "        [23., 67.],\n",
       "        [22., 75.],\n",
       "        [45., 23.],\n",
       "        [67., 22.],\n",
       "        [43., 45.],\n",
       "        [75., 67.],\n",
       "        [23., 23.],\n",
       "        [22., 22.],\n",
       "        [45., 45.],\n",
       "        [67., 67.]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4764.3604, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer \n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    for epoch in range(num_epochs):\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    # print the progress\n",
    "    if (epoch+1)%10==0:\n",
    "        print(epoch+1, num_epochs, loss.backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 <bound method Tensor.backward of tensor(393.7314, grad_fn=<MseLossBackward0>)>\n"
     ]
    }
   ],
   "source": [
    "fit(100, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 94.4367, 129.8920],\n",
       "        [ 36.8577,  47.8858],\n",
       "        [ 64.7351,  96.6732],\n",
       "        [ 56.6378,  53.4684],\n",
       "        [ 86.4359, 123.2147],\n",
       "        [108.4423, 159.7804],\n",
       "        [ 63.5360,  84.0048],\n",
       "        [ 31.2446,  18.5998],\n",
       "        [ 59.3353,  72.5468],\n",
       "        [110.6200, 141.5076],\n",
       "        [ 36.8577,  47.8858],\n",
       "        [ 35.2639,  45.8186],\n",
       "        [ 71.9213,  93.3634],\n",
       "        [106.9849, 138.8409]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[75., 75.],\n",
       "        [23., 23.],\n",
       "        [22., 22.],\n",
       "        [75., 45.],\n",
       "        [23., 67.],\n",
       "        [22., 75.],\n",
       "        [45., 23.],\n",
       "        [67., 22.],\n",
       "        [43., 45.],\n",
       "        [75., 67.],\n",
       "        [23., 23.],\n",
       "        [22., 22.],\n",
       "        [45., 45.],\n",
       "        [67., 67.]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download training dataset \n",
    "dataset = MNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.MNIST"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', train=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28>, 5)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22a94460710>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[6]\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image to transform\n",
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "datasets = MNIST(root='data/',\n",
    "                 train=True,\n",
    "                 transform = transform)\n",
    "\n",
    "# mnist_dataset = MNIST(root='data/', train=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label = datasets[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22a8e3273d0>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaI0lEQVR4nO3df2jU9x3H8dfVH1d1lytBk7vUmGVF202dpWrVYP3R1cxApf4oWMtGZEPa+YOJ/cGsDNNBjdgpRdI6V0amW239Y9a6KdUMTXRkijpdRYtYjDOdCcFM72LUSMxnf4hHz1j1e975vkueD/iCufu+vY/ffuvTby75xueccwIAwMBD1gsAAHRfRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJjpab2AW3V0dOjcuXMKBALy+XzWywEAeOScU0tLi/Ly8vTQQ3e+1km7CJ07d075+fnWywAA3Kf6+noNHDjwjvuk3afjAoGA9RIAAElwL3+fpyxCH3zwgQoLC/Xwww9r5MiR2rdv3z3N8Sk4AOga7uXv85REaPPmzVq8eLGWLVumI0eO6JlnnlFJSYnOnj2bipcDAGQoXyruoj1mzBg99dRTWrduXeyx73//+5o+fbrKy8vvOBuNRhUMBpO9JADAAxaJRJSVlXXHfZJ+JXTt2jUdPnxYxcXFcY8XFxertra20/5tbW2KRqNxGwCge0h6hM6fP6/r168rNzc37vHc3Fw1NjZ22r+8vFzBYDC28ZVxANB9pOwLE259Q8o5d9s3qZYuXapIJBLb6uvrU7UkAECaSfr3CfXv3189evTodNXT1NTU6epIkvx+v/x+f7KXAQDIAEm/Eurdu7dGjhypqqqquMerqqpUVFSU7JcDAGSwlNwxYcmSJfrpT3+qUaNGady4cfr973+vs2fP6tVXX03FywEAMlRKIjR79mw1NzfrN7/5jRoaGjRs2DDt2LFDBQUFqXg5AECGSsn3Cd0Pvk8IALoGk+8TAgDgXhEhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmelovAEgnPXr08DwTDAZTsJLkWLhwYUJzffv29Tzz+OOPe55ZsGCB55nf/va3nmfmzJnjeUaSrl696nlm5cqVnmfefvttzzNdBVdCAAAzRAgAYCbpESorK5PP54vbQqFQsl8GANAFpOQ9oaFDh+rvf/977ONEPs8OAOj6UhKhnj17cvUDALirlLwndOrUKeXl5amwsFAvvfSSTp8+/a37trW1KRqNxm0AgO4h6REaM2aMNm7cqJ07d+rDDz9UY2OjioqK1NzcfNv9y8vLFQwGY1t+fn6ylwQASFNJj1BJSYlmzZql4cOH67nnntP27dslSRs2bLjt/kuXLlUkEolt9fX1yV4SACBNpfybVfv166fhw4fr1KlTt33e7/fL7/enehkAgDSU8u8Tamtr05dffqlwOJzqlwIAZJikR+j1119XTU2N6urqdODAAb344ouKRqMqLS1N9ksBADJc0j8d9/XXX2vOnDk6f/68BgwYoLFjx2r//v0qKChI9ksBADJc0iP0ySefJPu3RJoaNGiQ55nevXt7nikqKvI8M378eM8zkvTII494npk1a1ZCr9XVfP31155n1q5d63lmxowZnmdaWlo8z0jSv//9b88zNTU1Cb1Wd8W94wAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAMz7nnLNexDdFo1EFg0HrZXQrTz75ZEJzu3fv9jzDf9vM0NHR4XnmZz/7meeZS5cueZ5JRENDQ0JzFy5c8Dxz8uTJhF6rK4pEIsrKyrrjPlwJAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwExP6wXA3tmzZxOaa25u9jzDXbRvOHDggOeZixcvep6ZPHmy5xlJunbtmueZP/3pTwm9Fro3roQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADPcwBT63//+l9DcG2+84Xnm+eef9zxz5MgRzzNr1671PJOoo0ePep6ZMmWK55nW1lbPM0OHDvU8I0m//OUvE5oDvOJKCABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAw43POOetFfFM0GlUwGLReBlIkKyvL80xLS4vnmfXr13uekaSf//znnmd+8pOfeJ75+OOPPc8AmSYSidz1/3muhAAAZogQAMCM5wjt3btX06ZNU15ennw+n7Zu3Rr3vHNOZWVlysvLU58+fTRp0iQdP348WesFAHQhniPU2tqqESNGqKKi4rbPr1q1SmvWrFFFRYUOHjyoUCikKVOmJPR5fQBA1+b5J6uWlJSopKTkts855/Tee+9p2bJlmjlzpiRpw4YNys3N1aZNm/TKK6/c32oBAF1KUt8TqqurU2Njo4qLi2OP+f1+TZw4UbW1tbedaWtrUzQajdsAAN1DUiPU2NgoScrNzY17PDc3N/bcrcrLyxUMBmNbfn5+MpcEAEhjKfnqOJ/PF/exc67TYzctXbpUkUgkttXX16diSQCANOT5PaE7CYVCkm5cEYXD4djjTU1Nna6ObvL7/fL7/clcBgAgQyT1SqiwsFChUEhVVVWxx65du6aamhoVFRUl86UAAF2A5yuhS5cu6auvvop9XFdXp6NHjyo7O1uDBg3S4sWLtWLFCg0ePFiDBw/WihUr1LdvX7388stJXTgAIPN5jtChQ4c0efLk2MdLliyRJJWWluqPf/yj3nzzTV25ckXz58/XhQsXNGbMGO3atUuBQCB5qwYAdAncwBRd0rvvvpvQ3M1/VHlRU1Pjeea5557zPNPR0eF5BrDEDUwBAGmNCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZriLNrqkfv36JTT317/+1fPMxIkTPc+UlJR4ntm1a5fnGcASd9EGAKQ1IgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMNzAFvuGxxx7zPPOvf/3L88zFixc9z+zZs8fzzKFDhzzPSNL777/veSbN/ipBGuAGpgCAtEaEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmOEGpsB9mjFjhueZyspKzzOBQMDzTKLeeustzzMbN270PNPQ0OB5BpmDG5gCANIaEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGG5gCBoYNG+Z5Zs2aNZ5nfvSjH3meSdT69es9z7zzzjueZ/773/96noENbmAKAEhrRAgAYMZzhPbu3atp06YpLy9PPp9PW7dujXt+7ty58vl8cdvYsWOTtV4AQBfiOUKtra0aMWKEKioqvnWfqVOnqqGhIbbt2LHjvhYJAOiaenodKCkpUUlJyR338fv9CoVCCS8KANA9pOQ9oerqauXk5GjIkCGaN2+empqavnXftrY2RaPRuA0A0D0kPUIlJSX66KOPtHv3bq1evVoHDx7Us88+q7a2ttvuX15ermAwGNvy8/OTvSQAQJry/Om4u5k9e3bs18OGDdOoUaNUUFCg7du3a+bMmZ32X7p0qZYsWRL7OBqNEiIA6CaSHqFbhcNhFRQU6NSpU7d93u/3y+/3p3oZAIA0lPLvE2publZ9fb3C4XCqXwoAkGE8XwldunRJX331Vezjuro6HT16VNnZ2crOzlZZWZlmzZqlcDisM2fO6K233lL//v01Y8aMpC4cAJD5PEfo0KFDmjx5cuzjm+/nlJaWat26dTp27Jg2btyoixcvKhwOa/Lkydq8ebMCgUDyVg0A6BK4gSmQIR555BHPM9OmTUvotSorKz3P+Hw+zzO7d+/2PDNlyhTPM7DBDUwBAGmNCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZriLNoBO2traPM/07On9BzW3t7d7nvnxj3/seaa6utrzDO4fd9EGAKQ1IgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMCM9zsOArhvP/zhDz3PvPjii55nRo8e7XlGSuxmpIk4ceKE55m9e/emYCWwwpUQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGG5gC3/D44497nlm4cKHnmZkzZ3qeCYVCnmcepOvXr3ueaWho8DzT0dHheQbpiyshAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMNzBF2kvkxp1z5sxJ6LUSuRnpd7/73YReK50dOnTI88w777zjeWbbtm2eZ9C1cCUEADBDhAAAZjxFqLy8XKNHj1YgEFBOTo6mT5+ukydPxu3jnFNZWZny8vLUp08fTZo0ScePH0/qogEAXYOnCNXU1GjBggXav3+/qqqq1N7eruLiYrW2tsb2WbVqldasWaOKigodPHhQoVBIU6ZMUUtLS9IXDwDIbJ6+MOHzzz+P+7iyslI5OTk6fPiwJkyYIOec3nvvPS1btiz2kyM3bNig3Nxcbdq0Sa+88kryVg4AyHj39Z5QJBKRJGVnZ0uS6urq1NjYqOLi4tg+fr9fEydOVG1t7W1/j7a2NkWj0bgNANA9JBwh55yWLFmi8ePHa9iwYZKkxsZGSVJubm7cvrm5ubHnblVeXq5gMBjb8vPzE10SACDDJByhhQsX6osvvtDHH3/c6Tmfzxf3sXOu02M3LV26VJFIJLbV19cnuiQAQIZJ6JtVFy1apG3btmnv3r0aOHBg7PGb31TY2NiocDgce7ypqanT1dFNfr9ffr8/kWUAADKcpysh55wWLlyoLVu2aPfu3SosLIx7vrCwUKFQSFVVVbHHrl27ppqaGhUVFSVnxQCALsPTldCCBQu0adMmffbZZwoEArH3eYLBoPr06SOfz6fFixdrxYoVGjx4sAYPHqwVK1aob9++evnll1PyBwAAZC5PEVq3bp0kadKkSXGPV1ZWau7cuZKkN998U1euXNH8+fN14cIFjRkzRrt27VIgEEjKggEAXYfPOeesF/FN0WhUwWDQehm4B9/2Pt+d/OAHP/A8U1FR4XnmiSee8DyT7g4cOOB55t13303otT777DPPMx0dHQm9FrquSCSirKysO+7DveMAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABgJqGfrIr0lZ2d7Xlm/fr1Cb3Wk08+6Xnme9/7XkKvlc5qa2s9z6xevdrzzM6dOz3PXLlyxfMM8CBxJQQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmOEGpg/ImDFjPM+88cYbnmeefvppzzOPPvqo55l0d/ny5YTm1q5d63lmxYoVnmdaW1s9zwBdEVdCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZbmD6gMyYMeOBzDxIJ06c8Dzzt7/9zfNMe3u755nVq1d7npGkixcvJjQHIDFcCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZnzOOWe9iG+KRqMKBoPWywAA3KdIJKKsrKw77sOVEADADBECAJjxFKHy8nKNHj1agUBAOTk5mj59uk6ePBm3z9y5c+Xz+eK2sWPHJnXRAICuwVOEampqtGDBAu3fv19VVVVqb29XcXGxWltb4/abOnWqGhoaYtuOHTuSumgAQNfg6Serfv7553EfV1ZWKicnR4cPH9aECRNij/v9foVCoeSsEADQZd3Xe0KRSESSlJ2dHfd4dXW1cnJyNGTIEM2bN09NTU3f+nu0tbUpGo3GbQCA7iHhL9F2zumFF17QhQsXtG/fvtjjmzdv1ne+8x0VFBSorq5Ov/71r9Xe3q7Dhw/L7/d3+n3Kysr09ttvJ/4nAACkpXv5Em25BM2fP98VFBS4+vr6O+537tw516tXL/eXv/zlts9fvXrVRSKR2FZfX+8ksbGxsbFl+BaJRO7aEk/vCd20aNEibdu2TXv37tXAgQPvuG84HFZBQYFOnTp12+f9fv9tr5AAAF2fpwg557Ro0SJ9+umnqq6uVmFh4V1nmpubVV9fr3A4nPAiAQBdk6cvTFiwYIH+/Oc/a9OmTQoEAmpsbFRjY6OuXLkiSbp06ZJef/11/fOf/9SZM2dUXV2tadOmqX///poxY0ZK/gAAgAzm5X0gfcvn/SorK51zzl2+fNkVFxe7AQMGuF69erlBgwa50tJSd/bs2Xt+jUgkYv55TDY2Nja2+9/u5T0hbmAKAEgJbmAKAEhrRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzaRch55z1EgAASXAvf5+nXYRaWlqslwAASIJ7+fvc59Ls0qOjo0Pnzp1TIBCQz+eLey4ajSo/P1/19fXKysoyWqE9jsMNHIcbOA43cBxuSIfj4JxTS0uL8vLy9NBDd77W6fmA1nTPHnroIQ0cOPCO+2RlZXXrk+wmjsMNHIcbOA43cBxusD4OwWDwnvZLu0/HAQC6DyIEADCTURHy+/1avny5/H6/9VJMcRxu4DjcwHG4geNwQ6Ydh7T7wgQAQPeRUVdCAICuhQgBAMwQIQCAGSIEADCTURH64IMPVFhYqIcfflgjR47Uvn37rJf0QJWVlcnn88VtoVDIelkpt3fvXk2bNk15eXny+XzaunVr3PPOOZWVlSkvL099+vTRpEmTdPz4cZvFptDdjsPcuXM7nR9jx461WWyKlJeXa/To0QoEAsrJydH06dN18uTJuH26w/lwL8chU86HjInQ5s2btXjxYi1btkxHjhzRM888o5KSEp09e9Z6aQ/U0KFD1dDQENuOHTtmvaSUa21t1YgRI1RRUXHb51etWqU1a9aooqJCBw8eVCgU0pQpU7rcfQjvdhwkaerUqXHnx44dOx7gClOvpqZGCxYs0P79+1VVVaX29nYVFxertbU1tk93OB/u5ThIGXI+uAzx9NNPu1dffTXusSeeeML96le/MlrRg7d8+XI3YsQI62WYkuQ+/fTT2McdHR0uFAq5lStXxh67evWqCwaD7ne/+53BCh+MW4+Dc86Vlpa6F154wWQ9VpqampwkV1NT45zrvufDrcfBucw5HzLiSujatWs6fPiwiouL4x4vLi5WbW2t0apsnDp1Snl5eSosLNRLL72k06dPWy/JVF1dnRobG+PODb/fr4kTJ3a7c0OSqqurlZOToyFDhmjevHlqamqyXlJKRSIRSVJ2drak7ns+3HocbsqE8yEjInT+/Hldv35dubm5cY/n5uaqsbHRaFUP3pgxY7Rx40bt3LlTH374oRobG1VUVKTm5mbrpZm5+d+/u58bklRSUqKPPvpIu3fv1urVq3Xw4EE9++yzamtrs15aSjjntGTJEo0fP17Dhg2T1D3Ph9sdBylzzoe0u4v2ndz6ox2cc50e68pKSkpivx4+fLjGjRunxx57TBs2bNCSJUsMV2avu58bkjR79uzYr4cNG6ZRo0apoKBA27dv18yZMw1XlhoLFy7UF198oX/84x+dnutO58O3HYdMOR8y4kqof//+6tGjR6d/yTQ1NXX6F0930q9fPw0fPlynTp2yXoqZm18dyLnRWTgcVkFBQZc8PxYtWqRt27Zpz549cT/6pbudD992HG4nXc+HjIhQ7969NXLkSFVVVcU9XlVVpaKiIqNV2Wtra9OXX36pcDhsvRQzhYWFCoVCcefGtWvXVFNT063PDUlqbm5WfX19lzo/nHNauHChtmzZot27d6uwsDDu+e5yPtztONxO2p4Phl8U4cknn3zievXq5f7whz+4EydOuMWLF7t+/fq5M2fOWC/tgXnttddcdXW1O336tNu/f797/vnnXSAQ6PLHoKWlxR05csQdOXLESXJr1qxxR44ccf/5z3+cc86tXLnSBYNBt2XLFnfs2DE3Z84cFw6HXTQaNV55ct3pOLS0tLjXXnvN1dbWurq6Ordnzx43btw49+ijj3ap4/CLX/zCBYNBV11d7RoaGmLb5cuXY/t0h/Phbschk86HjImQc869//77rqCgwPXu3ds99dRTcV+O2B3Mnj3bhcNh16tXL5eXl+dmzpzpjh8/br2slNuzZ4+T1GkrLS11zt34stzly5e7UCjk/H6/mzBhgjt27JjtolPgTsfh8uXLrri42A0YMMD16tXLDRo0yJWWlrqzZ89aLzupbvfnl+QqKytj+3SH8+FuxyGTzgd+lAMAwExGvCcEAOiaiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAz/wdVbyhNmNF0pQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_tensor[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22a8d6ff250>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_tensor[0, 10:15, 10:15], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(datasets, [50000, 10000])\n",
    "len(train_ds),len(val_ds)\n",
    "\n",
    "# train_ds, val_ds = random_split(mnist_dataset, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391, 79)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "# logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 784])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0014, -0.0175, -0.0127,  ..., -0.0181, -0.0323, -0.0044],\n",
       "        [ 0.0211,  0.0023,  0.0165,  ...,  0.0210,  0.0348,  0.0284],\n",
       "        [ 0.0159,  0.0076, -0.0193,  ...,  0.0088, -0.0354,  0.0270],\n",
       "        ...,\n",
       "        [ 0.0187, -0.0061,  0.0053,  ...,  0.0211,  0.0290, -0.0237],\n",
       "        [ 0.0254, -0.0042, -0.0169,  ..., -0.0187, -0.0168,  0.0262],\n",
       "        [-0.0234, -0.0253, -0.0267,  ..., -0.0254, -0.0066, -0.0259]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the training DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the training DataLoader\n",
    "# for images, labels in train_loader:\n",
    "#     print(labels.shape)\n",
    "#     print(images.shape)\n",
    "#     outputs = model(images)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        print(\"after\",xb.shape)\n",
    "        out = self.linear(xb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.linear.weight.shape)\n",
    "print(model.linear.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0034, -0.0322, -0.0257,  ...,  0.0336,  0.0238, -0.0037],\n",
       "         [ 0.0324, -0.0285,  0.0059,  ...,  0.0234, -0.0303, -0.0295],\n",
       "         [-0.0130, -0.0234, -0.0164,  ..., -0.0286, -0.0222,  0.0010],\n",
       "         ...,\n",
       "         [-0.0322, -0.0088,  0.0040,  ..., -0.0126, -0.0236, -0.0030],\n",
       "         [-0.0230, -0.0030, -0.0169,  ...,  0.0102,  0.0232, -0.0317],\n",
       "         [ 0.0205, -0.0352,  0.0161,  ...,  0.0189, -0.0115,  0.0223]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0285, -0.0060,  0.0249, -0.0334,  0.0124,  0.0184, -0.0125,  0.0046,\n",
       "         -0.0056, -0.0158], requires_grad=True)]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "after torch.Size([128, 784])\n",
      "output torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    break\n",
    "print(\"output\",outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0929, 0.1031, 0.0926, 0.1242, 0.1379, 0.0880, 0.0840, 0.0841, 0.0899,\n",
      "         0.1033],\n",
      "        [0.0675, 0.0803, 0.0937, 0.1410, 0.1072, 0.1468, 0.0643, 0.1028, 0.1024,\n",
      "         0.0940]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#app;y spftmax for reach output raw\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "print(probs[:2])\n",
    "len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000001192092896"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(probs[10]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 3, 8, 3, 3, 9, 3, 3, 8, 3, 3, 2, 3, 3, 3, 3, 1, 5, 3, 3, 3, 9, 3,\n",
      "        9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8, 9, 3, 2, 3, 8, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 4, 8, 9, 3, 3, 9, 3, 3, 1, 3, 3, 3, 3, 5, 8, 3, 9, 3,\n",
      "        3, 3, 7, 1, 3, 3, 3, 8, 3, 3, 1, 3, 4, 3, 3, 4, 3, 3, 3, 3, 5, 1, 9, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 8, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 9, 3, 3, 4, 9])\n",
      "tensor([0.1379, 0.1468, 0.1339, 0.1323, 0.1582, 0.1295, 0.1132, 0.1770, 0.1413,\n",
      "        0.1245, 0.1728, 0.1466, 0.1225, 0.1454, 0.1390, 0.1319, 0.1613, 0.1444,\n",
      "        0.1301, 0.1823, 0.1143, 0.1229, 0.1253, 0.1341, 0.1386, 0.1705, 0.1305,\n",
      "        0.1491, 0.1347, 0.1488, 0.1496, 0.1590, 0.1612, 0.1463, 0.1306, 0.1408,\n",
      "        0.1305, 0.1454, 0.1391, 0.1312, 0.1239, 0.1235, 0.1585, 0.1436, 0.1525,\n",
      "        0.1416, 0.1381, 0.1567, 0.1433, 0.1488, 0.1398, 0.1382, 0.1421, 0.1439,\n",
      "        0.1168, 0.1286, 0.1440, 0.1389, 0.1813, 0.1466, 0.1667, 0.1156, 0.1250,\n",
      "        0.1251, 0.1417, 0.1336, 0.1364, 0.1308, 0.1355, 0.1771, 0.1346, 0.1321,\n",
      "        0.1373, 0.1526, 0.1214, 0.1247, 0.1222, 0.1253, 0.1429, 0.1425, 0.1441,\n",
      "        0.1495, 0.1180, 0.1800, 0.1522, 0.1346, 0.1361, 0.1195, 0.1410, 0.1333,\n",
      "        0.1163, 0.1316, 0.1184, 0.1205, 0.1566, 0.1485, 0.1627, 0.1483, 0.1650,\n",
      "        0.1337, 0.1326, 0.1409, 0.1190, 0.1198, 0.1523, 0.1401, 0.1267, 0.1294,\n",
      "        0.1433, 0.1460, 0.1276, 0.1316, 0.1293, 0.1452, 0.1345, 0.1198, 0.1232,\n",
      "        0.1533, 0.1165, 0.1639, 0.1492, 0.1467, 0.1305, 0.1361, 0.1377, 0.1243,\n",
      "        0.1233, 0.1279], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find accuracy\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds ==labels).item()/ len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0859)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss functions\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3073, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training phase\n",
    "    for batch in train_loader:\n",
    "        # Generate predictions\n",
    "        # calculate loss\n",
    "        # compute gradients\n",
    "        # update weights\n",
    "        # reset gradients\n",
    "\n",
    "    # Validation phase\n",
    "    for batch in val_loader:\n",
    "        # Generate predictions\n",
    "        # Calculate loss\n",
    "        # Calculate metrics\n",
    "    # Calculate average validations loss & metrics\n",
    "\n",
    "    # Log epoch, loss & metrics for inspection\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_acces = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_acces).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc:{:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "\n",
    "model = MnistModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    ouputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(ouputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_laoder, val_loader, opt_func = torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        for batch in train_laoder:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3073034286499023, 'val_acc': 0.04736946150660515}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
